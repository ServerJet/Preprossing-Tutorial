{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the updated tutorial code with detailed explanations for each step, and the final code also outputs the preprocessed dataset at the end.\n",
    "\n",
    "### Step-by-Step Tutorial on Preprocessing\n",
    "\n",
    "This tutorial walks through data preprocessing using the 'AirQualityUCI.csv' dataset, covering missing value handling, scaling, encoding, feature engineering, and visualization.\n",
    "\n",
    "### Step 1: Import Libraries and Load the Dataset\n",
    "We start by importing essential libraries for data manipulation, preprocessing, and visualization, then load the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('AirQualityUCI.csv')\n",
    "\n",
    "# View the first few rows\n",
    "print(\"Initial dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Get information about the dataset\n",
    "print(\"\\nDataset Info:\")\n",
    "print(data.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(data.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:**\n",
    "- We use `pandas` for data handling, `numpy` for numerical operations, and `sklearn` for preprocessing.\n",
    "- The dataset is loaded using `pd.read_csv()`.\n",
    "- We inspect the first few rows with `data.head()` and the dataset’s structure with `data.info()`.\n",
    "- We check for missing values using `data.isnull().sum()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: Handling Missing and Invalid Values\n",
    "Here, we replace missing values using the mean strategy and filter out invalid values (e.g., -200).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values using the mean (CO(GT) column as an example)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data['CO(GT)'] = imputer.fit_transform(data[['CO(GT)']])\n",
    "\n",
    "# Drop rows with invalid values (-200)\n",
    "data = data[(data != -200).all(axis=1)]\n",
    "\n",
    "print(\"\\nDataset after handling missing and invalid values:\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Explanation:**\n",
    "- `SimpleImputer` replaces missing values in the 'CO(GT)' column with the column’s mean.\n",
    "- We filter out rows where any column has the value `-200`, indicating invalid data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3: Feature Scaling\n",
    "Scale numerical features using `StandardScaler` (for normalization) and `MinMaxScaler`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data using StandardScaler for CO, PT08.S1(CO), and C6H6(GT)\n",
    "scaler = StandardScaler()\n",
    "data[['CO(GT)', 'PT08.S1(CO)', 'C6H6(GT)']] = scaler.fit_transform(data[['CO(GT)', 'PT08.S1(CO)', 'C6H6(GT)']])\n",
    "\n",
    "# Apply Min-Max scaling for T, RH, and AH columns\n",
    "minmax_scaler = MinMaxScaler()\n",
    "data[['T', 'RH', 'AH']] = minmax_scaler.fit_transform(data[['T', 'RH', 'AH']])\n",
    "\n",
    "print(\"\\nDataset after scaling:\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Explanation:**\n",
    "- We normalize certain columns using `StandardScaler` to have mean 0 and variance 1, making them suitable for algorithms sensitive to the scale of input features.\n",
    "- We apply `MinMaxScaler` to scale other columns between 0 and 1 for consistency.\n",
    "\n",
    "### Step 4: Feature Engineering\n",
    "We create new features based on existing ones for additional insights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine 'Date' and 'Time' into a single datetime column and extract new features\n",
    "data['DateTime'] = pd.to_datetime(data['Date'] + ' ' + data['Time'])\n",
    "data['Hour'] = data['DateTime'].dt.hour\n",
    "data['Day'] = data['DateTime'].dt.day\n",
    "\n",
    "print(\"\\nDataset after feature engineering:\")\n",
    "print(data[['Date', 'Time', 'DateTime', 'Hour', 'Day']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Explanation:**\n",
    "- We convert the 'Date' and 'Time' columns into a single `datetime` column using `pd.to_datetime()`.\n",
    "- We then extract features like the hour and day from this combined `datetime` column.\n",
    "\n",
    "### Step 5: Data Visualization\n",
    "Visualize the distribution of 'CO(GT)' levels and the correlation between features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize CO levels distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['CO(GT)'], bins=30)\n",
    "plt.title('CO Levels Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap to explore relationships between variables\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(data.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Explanation:**\n",
    "- A histogram (`sns.histplot`) shows the distribution of 'CO(GT)' levels.\n",
    "- A correlation heatmap (`sns.heatmap`) displays relationships between features, highlighting which variables are positively or negatively correlated.\n",
    "\n",
    "### Step 6: Saving and Displaying the Preprocessed Data\n",
    "Finally, save the cleaned dataset and display the final version.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed data to a new CSV file\n",
    "data.to_csv('AirQualityUCI_preprocessed.csv', index=False)\n",
    "\n",
    "# Display the final dataset\n",
    "print(\"\\nFinal preprocessed dataset:\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Explanation:**\n",
    "- We save the preprocessed dataset as `AirQualityUCI_preprocessed.csv`.\n",
    "- We display the first few rows to confirm the preprocessing steps were applied correctly.\n",
    "\n",
    "### Consolidated Code\n",
    "\n",
    "Here's the complete executable code with all the above steps consolidated:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('AirQualityUCI.csv', sep=';')\n",
    "\n",
    "# Initial dataset exploration\n",
    "print(\"Initial dataset:\")\n",
    "print(data.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(data.info())\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Handle missing and invalid values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data['CO(GT)'] = imputer.fit_transform(data[['CO(GT)']])\n",
    "data = data[(data != -200).all(axis=1)]\n",
    "\n",
    "print(\"\\nDataset after handling missing and invalid values:\")\n",
    "print(data.head())\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "data[['CO(GT)', 'PT08.S1(CO)', 'C6H6(GT)']] = scaler.fit_transform(data[['CO(GT)', 'PT08.S1(CO)', 'C6H6(GT)']])\n",
    "minmax_scaler = MinMaxScaler()\n",
    "data[['T', 'RH', 'AH']] = minmax_scaler.fit_transform(data[['T', 'RH', 'AH']])\n",
    "\n",
    "print(\"\\nDataset after scaling:\")\n",
    "print(data.head())\n",
    "\n",
    "# Feature engineering\n",
    "data['DateTime'] = pd.to_datetime(data['Date'] + ' ' + data['Time'])\n",
    "data['Hour'] = data['DateTime'].dt.hour\n",
    "data['Day'] = data['DateTime'].dt.day\n",
    "\n",
    "print(\"\\nDataset after feature engineering:\")\n",
    "print(data[['Date', 'Time', 'DateTime', 'Hour', 'Day']].head())\n",
    "\n",
    "# Data visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['CO(GT)'], bins=30)\n",
    "plt.title('CO Levels Distribution')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(data.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Save the preprocessed data and display final dataset\n",
    "data.to_csv('AirQualityUCI_preprocessed.csv', index=False)\n",
    "print(\"\\nFinal preprocessed dataset:\")\n",
    "print(data.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Air_Quality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
